


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings


# Ignorar advertencias innecesarias
warnings.filterwarnings('ignore')


# Cargar el dataset
file_path = f'../data/raw/{"forest_fires_df.csv"}'
data = pd.read_csv(file_path)


# Visualizar las primeras filas del dataset
print("Primeras filas del dataset:")
print(data.head())


# Información del dataset para verificar los tipos de datos y valores nulos
print("\nInformación del dataset:")
data.info()


# Revisar si hay valores nulos
print("\nValores nulos por columna:")
print(data.isnull().sum())


# Corregir potenciales valores nulos si los hay
data = data.dropna()


# Revisar y Eliminar duplicados
print(f"\nNúmero de filas duplicadas: {data.duplicated().sum()}")
data = data.drop_duplicates()
print(f"Número de filas después de eliminar duplicados: {data.shape[0]}")


# Detectar y manejar outliers en la variable 'area' usando el método del rango intercuartílico (IQR)
Q1 = data['area'].quantile(0.25)
Q3 = data['area'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR


# Filtrar las filas con valores fuera de los límites
data_filtered = data[(data['area'] >= lower_bound) & (data['area'] <= upper_bound)]
print(f"Cantidad de datos después de eliminar outliers: {data_filtered.shape[0]}")


# Transformar la variable 'area' (debido al sesgo hacia valores pequeños) usando una escala logarítmica log(1 + area) para mejorar la distribución
data_filtered['log_area'] = np.log1p(data_filtered['area'])


# Codificar variables categóricas usando One-Hot Encoding para 'month' y 'day'
categorical_features = ['month', 'day']
numerical_features = ['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']


# Definir el pipeline de preprocesamiento
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),  # Escalar características numéricas
        ('cat', OneHotEncoder(drop='first'), categorical_features)  # One-Hot Encoding para categóricas
    ])


# Separar las características y la variable objetivo
X = data_filtered.drop(columns=['area', 'log_area'])  # Características predictoras (sin 'area' ni 'log_area')
y = data_filtered['log_area']  # Variable objetivo (área quemada en escala logarítmica)


# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Crear el pipeline de transformación y modelado con Random Forest
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])


# Entrenar el modelo de Machine Learning (Random Forest Regressor)
model_pipeline.fit(X_train, y_train)


# Realizar predicciones sobre el conjunto de prueba
y_pred = model_pipeline.predict(X_test)


# Evaluar el modelo con métricas de rendimiento
rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error (RMSE)
r2 = r2_score(y_test, y_pred)  # Coeficiente de determinación R²


# Mostrar los resultados de las métricas de evaluación del modelo
print("\nEvaluación del modelo:")
print(f"RMSE: {rmse:.4f}")
print(f"R²: {r2:.4f}")


# Guardar los datos procesados en archivos CSV para su uso posterior
X_train.to_csv('X_train_processed.csv', index=False)
X_test.to_csv('X_test_processed.csv', index=False)
y_train.to_csv('y_train_processed.csv', index=False)
y_test.to_csv('y_test_processed.csv', index=False)

print("\nLos datos han sido procesados y guardados en archivos CSV.")


# Guardar el dataset final preparado
file_path = f'../data/processed/{"forest_fires_prepared_df.csv"}'
data_filtered.to_csv(file_path, index=False)



